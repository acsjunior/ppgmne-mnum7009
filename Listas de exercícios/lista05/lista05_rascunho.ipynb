{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df26d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b34b19",
   "metadata": {},
   "source": [
    "### Processo de relaxação:\n",
    "\n",
    "Seja o sistema linear, com $\\mathbf{A}$, onde $\\mathbf{A}$ e positiva definida.:\n",
    "\n",
    "$\\mathbf{Ax} + \\mathbf{b} = \\mathbf{0}$\n",
    "\n",
    "Portanto, o sistema tem uma única solução.\n",
    "\n",
    "Se $\\mathbf{v}$ é uma aproximação da solução, então o resíduo é:\n",
    "\n",
    "$\\mathbf{r} = \\mathbf{Av} + \\mathbf{b}$\n",
    "\n",
    "O objetivo do processo de relaxação é fazer com que o resíduo se anule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8c130",
   "metadata": {},
   "source": [
    "A Função quadrática $F(v)$ é dada por:\n",
    "\n",
    "$F(v) = \\dfrac{1}{2}(\\mathbf{Av}, \\mathbf{v}) + (\\mathbf{b}, \\mathbf{v})$\n",
    "\n",
    "Em que $(\\mathbf{A}, \\mathbf{v})$ e $(\\mathbf{b}, \\mathbf{v})$ são os produtos escalares.\n",
    "\n",
    "Assim sendo:\n",
    "\n",
    "$F(v) = \\dfrac{1}{2} \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n a_{ij}v_iv_j + \\sum\\limits_{i=1}^n b_iv_i$\n",
    "\n",
    "Em que:\n",
    "\n",
    "$\\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n =$\n",
    "\n",
    "$ = a_{11}v_{1}v_{1} + a_{12}v_{1}v_{2} + \\ldots + a_{1n}v_{1}v_{n}$\n",
    "\n",
    "$+ a_{21}v_{2}v_{1} + a_{22}v_{2}v_{2} + \\ldots + a_{2n}v_{2}v_{n}$\n",
    "\n",
    "$\\cdots$\n",
    "\n",
    "$+ a_{n1}v_{n}v_{1} + a_{n2}v_{n}v_{2} + \\ldots + a_{nn}v_{n}v_{n}$\n",
    "\n",
    "e:\n",
    "\n",
    "$\\sum\\limits_{i=1}^n b_iv_i = b_1v_1 + b_2v_2 + \\ldots + b_nv_n$\n",
    "\n",
    "Portanto:\n",
    "\n",
    "$\\dfrac{\\partial \\sum_{i=1}^n \\sum_{j=1}^n a_{ij}v_iv_j}{\\partial v_i} = 2 \\sum\\limits_{j=1}^n a_{ij}v_i$\n",
    "\n",
    "Desde que $\\mathbf{A}$ é simétrica, e:\n",
    "\n",
    "\n",
    "$\\dfrac{\\partial \\sum_{i=1}^n b_iv_i}{\\partial v_i} = b_i$\n",
    "\n",
    "Logo, podemos escrever que:\n",
    "\n",
    "$\\dfrac{\\partial F(v)}{\\partial v_i} = \\dfrac{1}{2} \\cdot 2 \\sum\\limits_{j=1}^n a_{ij}v_j + b_i$\n",
    "\n",
    "$\\dfrac{\\partial F(v)}{\\partial v_i} = \\sum\\limits_{j=1}^n a_{ij}v_j + b_i$\n",
    "\n",
    "Agora:\n",
    "\n",
    "$\\nabla F(v) = \\left(\\dfrac{\\partial F(v)}{\\partial v_1}, \\dfrac{\\partial F(v)}{\\partial v_2}, \\ldots,  \\dfrac{\\partial F(v)}{\\partial v_n} \\right)$\n",
    "\n",
    "Portanto:\n",
    "\n",
    "$\\nabla F(v) = 0 \\Leftrightarrow \\dfrac{\\partial F(v)}{\\partial v_i} = 0$\n",
    "\n",
    "$\\nabla F(v) = 0 \\Leftrightarrow \\sum\\limits_{j=1}^n a_{ij}v_j + b_i = 0$\n",
    "\n",
    "Assim, temos que:\n",
    "\n",
    "$\\mathbf{Av} + \\mathbf{b} = 0 = \\nabla F(v)$\n",
    "\n",
    "Como $\\mathbf{Av} + \\mathbf{b} = \\mathbf{r}$, concluímos que $\\nabla F(v) = r$. Portanto, quando $\\nabla F(v) = 0$, teremos $r = 0$\n",
    "\n",
    "O objetivo é tentar anular o resíduo na direção do gradiente\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa38581",
   "metadata": {},
   "source": [
    "### Método dos Gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ca257",
   "metadata": {},
   "source": [
    "Seja o sistema linear, com $\\mathbf{A}$ positiva definida:\n",
    "\n",
    "$\\mathbf{Ax} + \\mathbf{b} = \\mathbf{0}$\n",
    "\n",
    "Construímos a função quadrática $F(v).$\n",
    "\n",
    "A solução do sistema dado coincide com o ponto de mínimo de $F(v)$, em que $\\nabla F(v) = Av + b = r$\n",
    "\n",
    "A direção $p$ de relaxação é dada por:\n",
    "\n",
    "$p^{(k)} = -r^{(k-1)}$\n",
    "\n",
    "Esta direção é dirigida para o ponto de mínimo.\n",
    "\n",
    "Todo processo iterativo onde a direção $p$ de relaxação é a do resíduo em sentido oposto é chamado Método dos Gradientes.\n",
    "\n",
    "Temos que:\n",
    "\n",
    "$t_{min} = -\\dfrac{(r,p)}{(Ap, p)} = -\\dfrac{(r^{(k-1)}, p^{(k)})}{(Ap^{(k)}, p^{(k)})}$\n",
    "\n",
    "$t_{min} = -\\dfrac{(r,p)}{(Ap, p)} = -\\dfrac{(r^{(k-1)}, r^{(k-1)})}{(Ap^{(k)}, r^{(k-1)})}$\n",
    "\n",
    "Ao usar $t_{min}$, o novo resíduo é ortogonal à direção de relaxação. Portanto, neste processo os resíduos consecutivos são ortogonais. Isto significa que:\n",
    "\n",
    "$(r^{(k)}, r^{(k-1)}) = 0$\n",
    "\n",
    "Assim sendo, temos que:\n",
    "\n",
    "$v^{(k)} = v^{(k-1)} + tp^{(k)}$\n",
    "\n",
    "$v^{(k)} = v^{(k-1)} - t_{min}r^{(k-1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea752b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente_old(A, b, x0, TOL=10**(-4), K_MAX = 100):\n",
    "    k = 0\n",
    "    while k < K_MAX:\n",
    "        \n",
    "        if k == 0:\n",
    "            r = np.matmul(A, x0) + b\n",
    "        else:\n",
    "            r = r - t_min*Ar\n",
    "\n",
    "        Ar = np.matmul(A, r)\n",
    "        t_min = np.dot(r.T[0], r.T[0]) / np.dot(Ar.T[0], r.T[0])\n",
    "\n",
    "        x0_old = copy(x0)\n",
    "        x0 = x0 - t_min*r\n",
    "\n",
    "        err = np.linalg.norm(x0 - x0_old, ord=np.infty) / np.linalg.norm(x0, ord=np.infty)\n",
    "        if err < TOL:\n",
    "            break\n",
    "\n",
    "        k += 1\n",
    "        \n",
    "    print(f'Resíduo: {r.T}')\n",
    "    print(f'Erro relativo: {err}')\n",
    "    print('\\n')\n",
    "    print(f'Solução aproximada com {k+1} iterações e tolerância de {TOL}:')\n",
    "    return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8fc963",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[10,1,0], [1,10,1], [0,1,10]])\n",
    "b = np.array([[-11], [-11], [-1]])\n",
    "x0 = np.array([[0], [0], [0]])\n",
    "tol = 10**(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1936a48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.],\n",
       "       [-1.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solução exata (para fins de conferência)\n",
    "np.matmul(np.linalg.inv(A), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a583a14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resíduo: [[-0.0857461   0.00445434  0.89420935]]\n",
      "Erro relativo: 0.08927229204397354\n",
      "\n",
      "\n",
      "Solução aproximada com 2 iterações e tolerância de 0.1:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.00077186e+00],\n",
       "       [9.91759863e-01],\n",
       "       [8.59247324e-04]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradiente_old(A, b, x0, tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2f536",
   "metadata": {},
   "source": [
    "### Método dos Gradientes Conjugados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a3d700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente_conjugado_old(A, b, x0, TOL=10**(-4), K_MAX = 100):\n",
    "    k = 0\n",
    "    while k < K_MAX:\n",
    "    \n",
    "        if k == 0:\n",
    "            r_old = np.matmul(A, x0) + b\n",
    "            Ar = np.matmul(A, r_old)\n",
    "            p = copy(-r_old)\n",
    "            q = np.dot(r_old.T[0], r_old.T[0]) / np.dot(Ar.T[0], r_old.T[0])\n",
    "            x0_old = copy(x0)\n",
    "            x0 = x0 - q*r_old\n",
    "            r = np.matmul(A, x0) + b\n",
    "\n",
    "        else:\n",
    "            alpha = np.dot(r.T[0], r.T[0]) / np.dot(r_old.T[0], r_old.T[0])\n",
    "            p = -r + alpha*p\n",
    "            Ap = np.matmul(A, p)\n",
    "            q = np.dot(r.T[0], r.T[0]) / np.dot(Ap.T[0], p.T[0])\n",
    "            x0_old = copy(x0)\n",
    "            x0 = x0 + q*p\n",
    "            r_old = copy(r)\n",
    "            r = r_old + q*Ap\n",
    "         \n",
    "        err = np.linalg.norm(x0 - x0_old, ord=np.infty) / np.linalg.norm(x0, ord=np.infty)\n",
    "        if err < TOL:\n",
    "            break\n",
    "            \n",
    "        k += 1\n",
    "    print(f'Resíduo: {r.T}')\n",
    "    print(f'Erro relativo: {err}')\n",
    "    print('\\n')\n",
    "    print(f'Solução aproximada com {k+1} iterações e tolerância de {TOL}:')\n",
    "    return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c0f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[10,1,0], [1,10,1], [0,1,10]])\n",
    "b = np.array([[-11], [-11], [-1]])\n",
    "x0 = np.array([[0], [0], [0]])\n",
    "tol = 10**(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "504379d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resíduo: [[ 6.93889390e-18 -6.93889390e-18 -1.30104261e-17]]\n",
      "Erro relativo: 0.004578555248150208\n",
      "\n",
      "\n",
      "Solução aproximada com 3 iterações e tolerância de 0.01:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00],\n",
       "       [1.00000000e+00],\n",
       "       [1.19262239e-17]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradiente_conjugado_old(A, b, x0, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54cb7f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4,1,-1, 0], [1,1,-1,0], [-1,-1,5, 2], [0,0,2,4]])\n",
    "b = np.array([[-7], [-8], [4], [-6]])\n",
    "x0 = np.array([[0], [0], [0], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1202b167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resíduo: [[ 1.55602179e-16 -1.49914628e-16 -1.19964579e-16  1.28019202e-16]]\n",
      "Erro relativo: 3.202566417187952e-17\n",
      "\n",
      "\n",
      "Solução aproximada com 5 iterações e tolerância de 0.0001:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.33333333],\n",
       "       [ 8.66666667],\n",
       "       [ 0.33333333],\n",
       "       [ 1.33333333]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradiente_conjugado_old(A, b, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4afe0962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33333333],\n",
       "       [-8.66666667],\n",
       "       [-0.33333333],\n",
       "       [-1.33333333]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solução exata:\n",
    "np.matmul(np.linalg.inv(A), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f03f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prof. Thadeu Penna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798aada",
   "metadata": {},
   "source": [
    "### Método do Gradiente\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\mathbf{Ax} = \\mathbf{b}$\n",
    "\n",
    "$\\mathbf{r}^{(0)} = \\mathbf{b} - \\mathbf{Ax}^{(0)}$\n",
    "\n",
    "$\\mathbf{x}^{(1)} = \\mathbf{x}^{(0)} + \\lambda \\mathbf{v}$\n",
    "\n",
    "$\\mathbf{r}^{(1)} = \\mathbf{b} - \\mathbf{Ax}^{(1)}$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Como escolher $\\lambda \\mathbf{v}$ de modo que $\\mathbf{r}^{(k+1)} \\lt \\mathbf{r}^{(k)}$?\n",
    "\n",
    "$\\nabla f(\\mathbf{x}) = \\mathbf{Ax} - \\mathbf{b}$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\nabla f(\\mathbf{x}) = \\mathbf{Ax} - \\mathbf{b} = 0$ é um ponto crítico (máximo, mínimo ou inflexão)\n",
    "A\n",
    "<br>\n",
    "\n",
    "Se a matriz $\\mathbf{H}$ (Hessiano - matriz das derivadas segundas) = $\\mathbf{A}$ é definida positiva, então $\\mathbf{Ax} - \\mathbf{b}$ é mínimo de $f(\\mathbf{x})$\n",
    "\n",
    "<hr>\n",
    "\n",
    "O gradiente é a direção de maior crescimento da função. Como o objetivo é encontrar o mínimo, então utiliza-se a direção do gradiente, porém no sentido contrário.\n",
    "\n",
    "<br>\n",
    "\n",
    "Portanto,\n",
    "\n",
    "$\\mathbf{x}^{(1)} = \\mathbf{x}^{(0)} + \\lambda \\mathbf{v}$\n",
    "\n",
    "Em que:\n",
    "\n",
    "$\\mathbf{v} = -\\nabla f(\\mathbf{x})$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\nabla f(\\mathbf{x}^{(0)}) = \\mathbf{Ax}^{(0)} - \\mathbf{b} = -\\mathbf{r}^{(0)}$\n",
    "\n",
    "$\\mathbf{x}^{(1)} = \\mathbf{x}^{(0)} + \\lambda \\mathbf{r}^{(0)}$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\mathbf{r}^{0} e \\nabla f(\\mathbf{x}^{(1)})$ são ortogonais\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\nabla f(\\mathbf{x}^{(1)}) = \\mathbf{Ax}^{(1)} - \\mathbf{b} = - \\mathbf{r}^{(1)}$\n",
    "\n",
    "Dada a necessidade de $\\mathbf{r}^{(1)}$ e $\\mathbf{r}^{(0)}$ serem ortogonais:\n",
    "\n",
    "$\\mathbf{r}^{(1)} \\cdot \\mathbf{r}^{(0)} = 0$\n",
    "\n",
    "$(\\mathbf{b} - \\mathbf{Ax}^{(1)}) \\cdot \\mathbf{r}^{(0)} = 0$\n",
    "\n",
    "$(\\mathbf{b} - \\mathbf{A}(\\mathbf{x}^{(0)} + \\lambda \\mathbf{r}^{(0)})) \\cdot \\mathbf{r}^{(0)} = 0$\n",
    "\n",
    "$(\\mathbf{b} - \\mathbf{A}(\\mathbf{x}^{(0)})  \\cdot \\mathbf{r}^{(0)} - \\lambda(\\mathbf{Ar}^{(0)}) \\cdot \\mathbf{r}^{(0)} = 0$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$(\\mathbf{b} - \\mathbf{A}(\\mathbf{x}^{(0)})  \\cdot \\mathbf{r}^{(0)} = \\lambda(\\mathbf{Ar}^{(0)}) \\cdot \\mathbf{r}^{(0)}$\n",
    "\n",
    "$\\mathbf{r}^{(0)}  \\cdot \\mathbf{r}^{(0)} = \\lambda(\\mathbf{Ar}^{(0)}) \\cdot \\mathbf{r}^{(0)}$\n",
    "\n",
    "$\\lambda = \\dfrac{\\mathbf{r}^{(0)}  \\cdot \\mathbf{r}^{(0)}}{\\mathbf{r}^{(0)} \\cdot \\mathbf{A} \\cdot \\mathbf{r}^{(0)}}$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Vantagem do método do gradiente: ele é menos sensível ao chute inicial\n",
    "\n",
    "Entretanto, o tamanho do passo vai ficando menor a cada iteração, á medida que se aproxima da raíz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f15989",
   "metadata": {},
   "source": [
    "Notação de produto interno:\n",
    "\n",
    "$\\mathbf{x} \\cdot \\mathbf{y} = \\mathbf{x}^T\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c653c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "\n",
    "A = np.array([[10,1,0], [1,10,1], [0,1,10]])\n",
    "b = np.array([[-11], [-11], [-1]])\n",
    "x0 = np.array([[0], [0], [0]])\n",
    "tol = 10**(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63617291",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(np.linalg.inv(A), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1df6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente(A, b, x0, TOL=10**(-4), K_MAX=100):\n",
    "\n",
    "    def calc_res(A, b, x):\n",
    "        return b - np.matmul(A, x)\n",
    "\n",
    "    def calc_step(r, A):\n",
    "        num_lamb = np.dot(r.T[0], r.T[0])\n",
    "        den_lamb = np.dot(np.dot(r.T[0], A), r.T[0])\n",
    "        return num_lamb/den_lamb\n",
    "\n",
    "    def calc_err(x_new, x0):\n",
    "        return np.linalg.norm(x_new - x0, ord=np.infty) / np.linalg.norm(x_new, ord=np.infty)\n",
    "\n",
    "    res = calc_res(A, b, x0)\n",
    "    k = 0\n",
    "    while k < K_MAX:\n",
    "\n",
    "        step = calc_step(res, A)\n",
    "        x_new = x0 + step*res\n",
    "        res = calc_res(A, b, x_new)\n",
    "        err = calc_err(x_new, x0)\n",
    "\n",
    "        if err >= TOL:\n",
    "            x0 = copy(x_new)\n",
    "            k += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    print(f'Resíduo: {res.T[0]}')\n",
    "    print(f'Erro relativo: {err}')\n",
    "    print('\\n')\n",
    "    print(f'Solução aproximada com {k+1} iterações e tolerância de {TOL}:')\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee66c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradiente(A, b, x0, tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd5814",
   "metadata": {},
   "source": [
    "### Método do Gradiente Conjugado\n",
    "\n",
    "<br>\n",
    "\n",
    "Na primeira iteração o método é idêntico ao do Gradiente (embora algumas notações estejam diferentes):\n",
    "\n",
    "$\\mathbf{d}^{(0)} = \\mathbf{r}^{(0)} = \\mathbf{b} - \\mathbf{Ax}^{(0)}$\n",
    "\n",
    "$\\mathbf{x}^{(1)} = \\mathbf{x}^{(0)} + \\alpha^{(0)} \\mathbf{d}^{(0)}$\n",
    "\n",
    "Em que:\n",
    "\n",
    "$\\alpha^{(0)} = \\dfrac{\\mathbf{r}^{(0)} \\cdot \\mathbf{d}^{(0)}}{\\mathbf{d}^{(0)} \\cdot \\mathbf{A} \\cdot \\mathbf{d}^{(0)}}$\n",
    "\n",
    "$\\mathbf{r}^{(1)} = \\mathbf{b} - \\mathbf{Ax}^{(1)} = \\mathbf{r}^{(0)} - \\alpha^{(0)} \\mathbf{A} \\cdot \\mathbf{d}^{(0)}$\n",
    "\n",
    "<br>\n",
    "\n",
    "Como atualizar $\\mathbf{d}^{(1)}?$\n",
    "\n",
    "O objetivo é tomar uma direção conjugada (ou A-ortogonal)\n",
    "\n",
    "$\\mathbf{d}^{(k+1)} \\cdot \\mathbf{A} \\cdot \\mathbf{d}^{(i)} = 0$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\mathbf{d}^{(1)} = \\mathbf{r}^{(1)} + \\beta^{(1)} \\mathbf{d}^{(0)}$\n",
    "\n",
    "$\\mathbf{d}^{(1)} \\cdot \\mathbf{A} \\cdot \\mathbf{d}^{(0)} = (\\mathbf{r}^{(1)} + \\beta^{(1)} \\mathbf{d}^{(0)}) \\cdot \\mathbf{A} \\cdot \\mathbf{d}^{(0)}$\n",
    "\n",
    "Em que:\n",
    "\n",
    "$\\beta^{(1)} = -\\dfrac{\\mathbf{r}^{(1)} \\cdot \\mathbf{A} \\cdot \\mathbf{d}^{(0)}}{\\mathbf{d}^{(0)} \\cdot \\mathbf{A} \\cdot \\mathbf{d}^{(0)}}$\n",
    "\n",
    "Ou, de forma simplificada:\n",
    "\n",
    "$\\beta^{(1)} = \\dfrac{\\mathbf{r}^{(1)} \\cdot \\mathbf{r}^{(1)}}{\\mathbf{r}^{(0)} \\cdot \\mathbf{r}^{(0)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce803f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente_conjugado(A, b, x0, TOL=10**(-4), K_MAX=100):\n",
    "\n",
    "    def calc_res(A, b, x):\n",
    "        return b - np.matmul(A, x)\n",
    "\n",
    "    def calc_step(d, r, A):\n",
    "        num_step = np.dot(r.T[0], r.T[0])\n",
    "        den_step = np.dot(np.dot(d.T[0], A), d.T[0])\n",
    "        return num_step/den_step\n",
    "\n",
    "    def calc_err(x_new, x0):\n",
    "        return np.linalg.norm(x_new - x0, ord=np.infty) / np.linalg.norm(x_new, ord=np.infty)\n",
    "\n",
    "    direct = res = calc_res(A, b, x0)\n",
    "    k = 0\n",
    "    while k < K_MAX:\n",
    "\n",
    "        step = calc_step(direct, res, A)\n",
    "        x_new = x0 + step*res\n",
    "        err = calc_err(x_new, x0)\n",
    "        \n",
    "        if err >= TOL:\n",
    "            x0 = copy(x_new)\n",
    "            res_old = copy(res)\n",
    "            res = calc_res(A, b, x_new)\n",
    "            beta = np.dot(res.T[0], res.T[0]) / np.dot(res_old.T[0], res_old.T[0])\n",
    "            dir_old = copy(direct)\n",
    "            direct = res + beta*dir_old\n",
    "            k += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    print(f'Resíduo: {res.T[0]}')\n",
    "    print(f'Erro relativo: {err}')\n",
    "    print('\\n')\n",
    "    print(f'Solução aproximada com {k+1} iterações e tolerância de {TOL}:')\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[10,1,0], [1,10,1], [0,1,10]])\n",
    "b = np.array([[-11], [-11], [-1]])\n",
    "x0 = np.array([[0], [0], [0]])\n",
    "tol = 10**(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e85623",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(np.linalg.inv(A), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ed408",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradiente_conjugado(A, b, x0, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a2dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
