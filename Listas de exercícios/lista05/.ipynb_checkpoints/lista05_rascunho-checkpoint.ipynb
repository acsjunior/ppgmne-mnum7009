{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22869b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969539d6",
   "metadata": {},
   "source": [
    "### Processo de relaxação:\n",
    "\n",
    "Seja o sistema linear, com $\\mathbf{A}$, onde $\\mathbf{A}$ e positiva definida.:\n",
    "\n",
    "$\\mathbf{Ax} + \\mathbf{b} = \\mathbf{0}$\n",
    "\n",
    "Portanto, o sistema tem uma única solução.\n",
    "\n",
    "Se $\\mathbf{v}$ é uma aproximação da solução, então o resíduo é:\n",
    "\n",
    "$\\mathbf{r} = \\mathbf{Av} + \\mathbf{b}$\n",
    "\n",
    "O objetivo do processo de relaxação é fazer com que o resíduo se anule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd658f0",
   "metadata": {},
   "source": [
    "A Função quadrática $F(v)$ é dada por:\n",
    "\n",
    "$F(v) = \\dfrac{1}{2}(\\mathbf{Av}, \\mathbf{v}) + (\\mathbf{b}, \\mathbf{v})$\n",
    "\n",
    "Em que $(\\mathbf{A}, \\mathbf{v})$ e $(\\mathbf{b}, \\mathbf{v})$ são os produtos escalares.\n",
    "\n",
    "Assim sendo:\n",
    "\n",
    "$F(v) = \\dfrac{1}{2} \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n a_{ij}v_iv_j + \\sum\\limits_{i=1}^n b_iv_i$\n",
    "\n",
    "Em que:\n",
    "\n",
    "$\\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n =$\n",
    "\n",
    "$ = a_{11}v_{1}v_{1} + a_{12}v_{1}v_{2} + \\ldots + a_{1n}v_{1}v_{n}$\n",
    "\n",
    "$+ a_{21}v_{2}v_{1} + a_{22}v_{2}v_{2} + \\ldots + a_{2n}v_{2}v_{n}$\n",
    "\n",
    "$\\cdots$\n",
    "\n",
    "$+ a_{n1}v_{n}v_{1} + a_{n2}v_{n}v_{2} + \\ldots + a_{nn}v_{n}v_{n}$\n",
    "\n",
    "e:\n",
    "\n",
    "$\\sum\\limits_{i=1}^n b_iv_i = b_1v_1 + b_2v_2 + \\ldots + b_nv_n$\n",
    "\n",
    "Portanto:\n",
    "\n",
    "$\\dfrac{\\partial \\sum_{i=1}^n \\sum_{j=1}^n a_{ij}v_iv_j}{\\partial v_i} = 2 \\sum\\limits_{j=1}^n a_{ij}v_i$\n",
    "\n",
    "Desde que $\\mathbf{A}$ é simétrica, e:\n",
    "\n",
    "\n",
    "$\\dfrac{\\partial \\sum_{i=1}^n b_iv_i}{\\partial v_i} = b_i$\n",
    "\n",
    "Logo, podemos escrever que:\n",
    "\n",
    "$\\dfrac{\\partial F(v)}{\\partial v_i} = \\dfrac{1}{2} \\cdot 2 \\sum\\limits_{j=1}^n a_{ij}v_j + b_i$\n",
    "\n",
    "$\\dfrac{\\partial F(v)}{\\partial v_i} = \\sum\\limits_{j=1}^n a_{ij}v_j + b_i$\n",
    "\n",
    "Agora:\n",
    "\n",
    "$\\nabla F(v) = \\left(\\dfrac{\\partial F(v)}{\\partial v_1}, \\dfrac{\\partial F(v)}{\\partial v_2}, \\ldots,  \\dfrac{\\partial F(v)}{\\partial v_n} \\right)$\n",
    "\n",
    "Portanto:\n",
    "\n",
    "$\\nabla F(v) = 0 \\Leftrightarrow \\dfrac{\\partial F(v)}{\\partial v_i} = 0$\n",
    "\n",
    "$\\nabla F(v) = 0 \\Leftrightarrow \\sum\\limits_{j=1}^n a_{ij}v_j + b_i = 0$\n",
    "\n",
    "Assim, temos que:\n",
    "\n",
    "$\\mathbf{Av} + \\mathbf{b} = 0 = \\nabla F(v)$\n",
    "\n",
    "Como $\\mathbf{Av} + \\mathbf{b} = \\mathbf{r}$, concluímos que $\\nabla F(v) = r$. Portanto, quando $\\nabla F(v) = 0$, teremos $r = 0$\n",
    "\n",
    "O objetivo é tentar anular o resíduo na direção do gradiente\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6fc671",
   "metadata": {},
   "source": [
    "### Método dos Gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35dedb",
   "metadata": {},
   "source": [
    "Seja o sistema linear, com $\\mathbf{A}$ positiva definida:\n",
    "\n",
    "$\\mathbf{Ax} + \\mathbf{b} = \\mathbf{0}$\n",
    "\n",
    "Construímos a função quadrática $F(v).$\n",
    "\n",
    "A solução do sistema dado coincide com o ponto de mínimo de $F(v)$, em que $\\nabla F(v) = Av + b = r$\n",
    "\n",
    "A direção $p$ de relaxação é dada por:\n",
    "\n",
    "$p^{(k)} = -r^{(k-1)}$\n",
    "\n",
    "Esta direção é dirigida para o ponto de mínimo.\n",
    "\n",
    "Todo processo iterativo onde a direção $p$ de relaxação é a do resíduo em sentido oposto é chamado Método dos Gradientes.\n",
    "\n",
    "Temos que:\n",
    "\n",
    "$t_{min} = -\\dfrac{(r,p)}{(Ap, p)} = -\\dfrac{(r^{(k-1)}, p^{(k)})}{(Ap^{(k)}, p^{(k)})}$\n",
    "\n",
    "$t_{min} = -\\dfrac{(r,p)}{(Ap, p)} = -\\dfrac{(r^{(k-1)}, r^{(k-1)})}{(Ap^{(k)}, r^{(k-1)})}$\n",
    "\n",
    "Ao usar $t_{min}$, o novo resíduo é ortogonal à direção de relaxação. Portanto, neste processo os resíduos consecutivos são ortogonais. Isto significa que:\n",
    "\n",
    "$(r^{(k)}, r^{(k-1)}) = 0$\n",
    "\n",
    "Assim sendo, temos que:\n",
    "\n",
    "$v^{(k)} = v^{(k-1)} + tp^{(k)}$\n",
    "\n",
    "$v^{(k)} = v^{(k-1)} - t_{min}r^{(k-1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5401c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente(A, b, x0, TOL=10**(-4), K_MAX = 100):\n",
    "    k = 0\n",
    "    while k < K_MAX:\n",
    "        \n",
    "        if k == 0:\n",
    "            r = np.matmul(A, x0) + b\n",
    "        else:\n",
    "            r = r - t_min*Ar\n",
    "\n",
    "        Ar = np.matmul(A, r)\n",
    "        t_min = np.dot(r.T[0], r.T[0]) / np.dot(Ar.T[0], r.T[0])\n",
    "\n",
    "        x0_old = copy(x0)\n",
    "        x0 = x0 - t_min*r\n",
    "\n",
    "        err = np.linalg.norm(x0 - x0_old, ord=np.infty) / np.linalg.norm(x0, ord=np.infty)\n",
    "        if err < TOL:\n",
    "            break\n",
    "\n",
    "        k += 1\n",
    "        \n",
    "    print(f'Resíduo: {r.T}')\n",
    "    print(f'Erro relativo: {err}')\n",
    "    print('\\n')\n",
    "    print(f'Solução aproximada com {k+1} iterações e tolerância de {TOL}:')\n",
    "    return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a799cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[10,1,0], [1,10,1], [0,1,10]])\n",
    "b = np.array([[-11], [-11], [-1]])\n",
    "x0 = np.array([[0], [0], [0]])\n",
    "tol = 10**(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5291d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.],\n",
       "       [-1.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solução exata (para fins de conferência)\n",
    "np.matmul(np.linalg.inv(A), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11d8055b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resíduo: [[-0.0857461   0.00445434  0.89420935]]\n",
      "Erro relativo: 0.08927229204397354\n",
      "\n",
      "\n",
      "Solução aproximada com 2 iterações e tolerância de 0.1:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.00077186e+00],\n",
       "       [9.91759863e-01],\n",
       "       [8.59247324e-04]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradiente(A, b, x0, tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03abc2",
   "metadata": {},
   "source": [
    "### Método dos Gradientes Conjugados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d50ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente_conjugado(A, b, x0, TOL=10**(-4), K_MAX = 100):\n",
    "    k = 0\n",
    "    while k < K_MAX:\n",
    "    \n",
    "        if k == 0:\n",
    "            r_old = np.matmul(A, x0) + b\n",
    "            Ar = np.matmul(A, r_old)\n",
    "            p = copy(-r_old)\n",
    "            q = np.dot(r_old.T[0], r_old.T[0]) / np.dot(Ar.T[0], r_old.T[0])\n",
    "            x0_old = copy(x0)\n",
    "            x0 = x0 - q*r_old\n",
    "            r = np.matmul(A, x0) + b\n",
    "\n",
    "        else:\n",
    "            alpha = np.dot(r.T[0], r.T[0]) / np.dot(r_old.T[0], r_old.T[0])\n",
    "            p = -r + alpha*p\n",
    "            Ap = np.matmul(A, p)\n",
    "            q = np.dot(r.T[0], r.T[0]) / np.dot(Ap.T[0], p.T[0])\n",
    "            x0_old = copy(x0)\n",
    "            x0 = x0 + q*p\n",
    "            r_old = copy(r)\n",
    "            r = r_old + q*Ap\n",
    "         \n",
    "        err = np.linalg.norm(x0 - x0_old, ord=np.infty) / np.linalg.norm(x0, ord=np.infty)\n",
    "        if err < TOL:\n",
    "            break\n",
    "            \n",
    "        k += 1\n",
    "    print(f'Resíduo: {r.T}')\n",
    "    print(f'Erro relativo: {err}')\n",
    "    print('\\n')\n",
    "    print(f'Solução aproximada com {k+1} iterações e tolerância de {TOL}:')\n",
    "    return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bd066",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[10,1,0], [1,10,1], [0,1,10]])\n",
    "b = np.array([[-11], [-11], [-1]])\n",
    "x0 = np.array([[0], [0], [0]])\n",
    "tol = 10**(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b77be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradiente_conjugado(A, b, x0, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da07386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8522d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c951cb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e2ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c856cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prof. Thadeu Penna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb06a959",
   "metadata": {},
   "source": [
    "$\\mathbf{Ax} = \\mathbf{b}$\n",
    "\n",
    "$\\mathbf{r}^{(0)} = \\mathbf{b} - \\mathbf{Ax}^{(0)}$\n",
    "\n",
    "$\\mathbf{x}^{(1)} = \\mathbf{x}^{(0)} + \\lambda \\mathbf{v}$\n",
    "\n",
    "$\\mathbf{r}^{(1)} = \\mathbf{b} - \\mathbf{Ax}^{(1)}$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Como escolher $\\lambda \\mathbf{v}$ de modo que $\\mathbf{r}^{(k+1)} \\lt \\mathbf{r}^{(k)}$?\n",
    "\n",
    "$\\nabla f(\\mathbf{x}) = \\mathbf{Ax} - \\mathbf{b}$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\nabla f(\\mathbf{x}) = \\mathbf{Ax} - \\mathbf{b} = 0$ é um ponto crítico (máximo, mínimo ou inflexão)\n",
    "A\n",
    "<br>\n",
    "\n",
    "Se a matriz $\\mathbf{H}$ (Hessiano - matriz das derivadas segundas) = $\\mathbf{A}$ é definida positiva, então $\\mathbf{Ax} - \\mathbf{b}$ é mínimo de $f(\\mathbf{x})$\n",
    "\n",
    "<hr>\n",
    "\n",
    "O gradiente é a direção de maior crescimento da função. Como o objetivo é encontrar o mínimo, então utiliza-se a direção do gradiente, porém no sentido contrário.\n",
    "\n",
    "<br>\n",
    "\n",
    "Portanto,\n",
    "\n",
    "$\\mathbf{x}^{(1)} = \\mathbf{x}^{(0)} + \\lambda \\mathbf{v}$\n",
    "\n",
    "Em que:\n",
    "\n",
    "$\\mathbf{v} = -\\nabla f(\\mathbf{x})$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\nabla f(\\mathbf{x}^{(0)}) = \\mathbf{Ax}^{(0)} - \\mathbf{b} = -\\mathbf{r}^{(0)}$\n",
    "\n",
    "$\\mathbf{x}^{(1)} = \\mathbf{x}^{(0)} + \\lambda \\mathbf{r}^{(0)}$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\mathbf{r}^{0} e \\nabla f(\\mathbf{x}^{(1)})$ são ortogonais\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\nabla f(\\mathbf{x}^{(1)}) = \\mathbf{Ax}^{(1)} - \\mathbf{b} = - \\mathbf{r}^{(1)}$\n",
    "\n",
    "Dada a necessidade de $\\mathbf{r}^{(1)}$ e $\\mathbf{r}^{(0)}$ serem ortogonais:\n",
    "\n",
    "$\\mathbf{r}^{(1)} \\cdot \\mathbf{r}^{(0)} = 0$\n",
    "\n",
    "$(\\mathbf{b} - \\mathbf{Ax}^{(1)}) \\cdot \\mathbf{r}^{(0)} = 0$\n",
    "\n",
    "$(\\mathbf{b} - \\mathbf{A}(\\mathbf{x}^{(0)} + \\lambda \\mathbf{r}^{(0)})) \\cdot \\mathbf{r}^{(0)} = 0$\n",
    "\n",
    "$(\\mathbf{b} - \\mathbf{A}(\\mathbf{x}^{(0)})  \\cdot \\mathbf{r}^{(0)} - \\lambda(\\mathbf{Ar}^{(0)}) \\cdot \\mathbf{r}^{(0)} = 0$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$(\\mathbf{b} - \\mathbf{A}(\\mathbf{x}^{(0)})  \\cdot \\mathbf{r}^{(0)} = \\lambda(\\mathbf{Ar}^{(0)}) \\cdot \\mathbf{r}^{(0)}$\n",
    "\n",
    "$\\mathbf{r}^{(0)}  \\cdot \\mathbf{r}^{(0)} = \\lambda(\\mathbf{Ar}^{(0)}) \\cdot \\mathbf{r}^{(0)}$\n",
    "\n",
    "$\\lambda = \\dfrac{\\mathbf{r}^{(0)}  \\cdot \\mathbf{r}^{(0)}}{\\mathbf{r}^{(0)} \\cdot \\mathbf{A} \\cdot \\mathbf{r}^{(0)}}$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Vantagem do método do gradiente: ele é menos sensível ao chute inicial\n",
    "\n",
    "Entretanto, o tamanho do passo vai ficando menor a cada iteração, á medida que se aproxima da raíz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f85225",
   "metadata": {},
   "source": [
    "Notação de produto interno:\n",
    "\n",
    "$\\mathbf{x} \\cdot \\mathbf{y} = \\mathbf{x}^T\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b67fa75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[10,1,0], [1,10,1], [0,1,10]])\n",
    "b = np.array([[-11], [-11], [-1]])\n",
    "x0 = np.array([[0], [0], [0]])\n",
    "tol = 10**(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cb7193a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-11],\n",
       "       [-11],\n",
       "       [ -1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = b - np.matmul(A, x0)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f8caff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(res.T, res)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3a922de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(res.T[0], res.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dcc955cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2694"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.dot(res.T[0], A), res.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668ba28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab595a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d39c54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente(A, b, x0, TOL=10**(-4), K_MAX=100):\n",
    "\n",
    "    def calc_res(A, b, x):\n",
    "        return b - np.matmul(A, x)\n",
    "\n",
    "    def calc_step(r, A):\n",
    "        num_lamb = np.dot(r.T[0], r.T[0])\n",
    "        den_lamb = np.dot(np.dot(r.T[0], A), r.T[0])\n",
    "        return num_lamb/den_lamb\n",
    "\n",
    "    def calc_err(x_new, x0):\n",
    "        return np.linalg.norm(x_new - x0, ord=np.infty) / np.linalg.norm(x_new, ord=np.infty)\n",
    "\n",
    "    res = calc_res(A, b, x0)\n",
    "    k = 0\n",
    "    while k < K_MAX:\n",
    "\n",
    "        step = calc_step(res, A)\n",
    "        x_new = x0 + step*res\n",
    "        res = calc_res(A, b, x_new)\n",
    "        err = calc_err(x_new, x0)\n",
    "\n",
    "        if err >= TOL:\n",
    "            x0 = copy(x_new)\n",
    "            k += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    print(f'Resíduo: {res.T[0]}')\n",
    "    print(f'Erro relativo: {err}')\n",
    "    print('\\n')\n",
    "    print(f'Solução aproximada com {k+1} iterações e tolerância de {TOL}:')\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c464bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resíduo: [-0.0005215  -0.08077026  0.00035234]\n",
      "Erro relativo: 0.08927229204397354\n",
      "\n",
      "\n",
      "Solução aproximada com 2 iterações e tolerância de 0.1:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.00077186e+00],\n",
       "       [-9.91759863e-01],\n",
       "       [-8.59247324e-04]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradiente(A, b, x0, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9fd795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
